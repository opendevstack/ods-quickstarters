// See https://www.opendevstack.org/ods-documentation/ for usage and customization.

@Library('ods-jenkins-shared-library@@ods_git_ref@') _

odsComponentPipeline(
    imageStreamTag: '@ods_namespace@/jenkins-agent-airflow:@ods_image_tag@',
    componentId: 'airflow-worker',
    testResults : 'artifacts',
    branchToEnvironmentMapping: [
        'master': 'dev',
        // 'release/': 'test'
    ]
) { context ->
    stageDAGTest(context)
    stageUnitTest(context)
    stageBuild(context)
    odsComponentStageScanWithSonar(context)
    odsComponentStageBuildOpenShiftImage(context, [buildTimeoutMinutes: 25])
    stageTagImage(context)
    stagePublishDAGs(context)
}

def stageDAGTest(def context) {
    stage('DAG Integrity Tests') {
        sh 'sh test_dag_integrity.sh'
    }
}

def stageUnitTest(def context) {
    stage('Unit Tests') {
        sh "pip install -i ${context.nexusHostWithBasicAuth}/repository/pypi-all/simple --trusted-host ${context.nexusHostWithoutScheme} -r src/requirements.txt --user"
        sh 'sh test_all.sh'
    }
}

def stageBuild(def context) {
    stage('Build') {
        sh 'sh build.sh'
    }
}

def stageTagImage(def context) {
    stage('Tag build') {
        if (!context.environment) {
            println("Skipping for empty environment ...")
            return
        }
        sh(
            script: "oc -n ${context.targetProject} tag ${context.componentId}:${context.tagversion} ${context.componentId}:latest",
            label: "Update latest tag of is/${context.componentId} to ${context.tagversion}"
        )
    }
}

def stagePublishDAGs(def context) {
    stage('Publish DAGs') {
        def airflow_webserver_info = sh(returnStdout: true, script: "oc get pods --sort-by=.status.startTime --no-headers -l component=airflow-webserver -n ${context.targetProject} | tail -n1").trim().split(/\s+/)
        def airflow_scheduler_info = sh(returnStdout: true, script: "oc get pods --sort-by=.status.startTime --no-headers -l component=airflow-scheduler -n ${context.targetProject} | tail -n1").trim().split(/\s+/)

        if (airflow_webserver_info[2] != "Running" || airflow_scheduler_info[2] != "Running") {
            error("Airflow cluster is not running or does not exist")
        }

        sh "oc rsync --no-perms=true --delete=true --exclude=.keep --exclude=__pycache__ src/dags ${airflow_webserver_info[0]}:/opt/app-root/src/airflow/ -n ${context.targetProject}"
        sh "oc rsync --no-perms=true --delete=true --exclude=.keep --exclude=__pycache__ src/dags ${airflow_scheduler_info[0]}:/opt/app-root/src/airflow/ -n ${context.targetProject}"
        sh "oc rsync --no-perms=true --delete=true --exclude=.keep --exclude=__pycache__ src/dag_deps ${airflow_webserver_info[0]}:/opt/app-root/src/airflow/ -n ${context.targetProject}"
        sh "oc rsync --no-perms=true --delete=true --exclude=.keep --exclude=__pycache__ src/dag_deps ${airflow_scheduler_info[0]}:/opt/app-root/src/airflow/ -n ${context.targetProject}"
    }
}
